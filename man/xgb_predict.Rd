% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb_predict.R
\name{xgb_predict}
\alias{xgb_predict}
\title{Predict from an xgboost model at a given number of rounds, across resamples}
\usage{
xgb_predict(
  object,
  newdata = NULL,
  niter = NULL,
  fns = list(mean = mean, sd = stats::sd, se = se),
  add_data = TRUE,
  ...
)
}
\arguments{
\item{object}{an object output by \code{xgb_fit()}, which contains a \code{model} column.}

\item{newdata}{data.frame to predict, with the same variables as those used
for fitting (and possibly others). When NULL, predict the
validation data for each resample.}

\item{niter}{number of boosting iterations to use in the prediction.
Maps to the last bound of \code{iterationrange} in \code{xgboost::predict.xgb.Booster()}.}

\item{fns}{a named list of summary functions, to compute for the predictions
of each observation, across resamples. If NULL or an empty list,
just return the full predictions.}

\item{add_data}{boolean, whether to add the original data to the output
(defaults to TRUE which is practical to compute performance
metrics).}

\item{...}{passed to xgboost::predict.xgb.Booster()}
}
\value{
A tibble with columns
\itemize{
\item the grouping columns in \code{object}. Ungroup the object before \code{xgb_predict()}
if this is not the desired behaviour.
\item the predictions as
\itemize{
\item \verb{pred_***} where *** is a summary function (e.g. mean),
or
\item \code{pred} when no summary function is chosen.
}
\item the original data if \code{add_data} is TRUE.
}
}
\description{
Predict from an xgboost model at a given number of rounds, across resamples
}
\examples{
# fit models over 3 folds of cross-validation, with 6 rounds of boost each
fits <- resample_cv(mtcars, k=3) \%>\%
  xgb_fit(resp="mpg", expl=c("cyl", "hp", "qsec"),
          eta=0.1, max_depth=2, nrounds=30)
# compute the average predicted mpg over the 100 bootstraps, with 3 trees
res <- xgb_predict(fits, ntrees=20, fns=list(mean=mean))
res
# check that we have predicted all items in the dataset (should always be the
# case with cross validation)
nrow(res)
nrow(mtcars)
# compute Mean Squared Error at this tree number
sum(res$mpg-mean(res$mpg)^2)
sum((res$pred_mean - res$mpg)^2)/nrow(res)

1 - sum((res$mpg-res$pred_mean)^2)/sum((res$mpg-mean(res$mpg))^2)
cor(res$mpg, res$pred_mean)
MLmetrics::R2_Score(y_pred=res$pred_mean, y_true=res$mpg)


res <- resample_cv(mtcars, k=3) \%>\%
  param_grid(eta=c(0.1, 0.5)) \%>\%
  xgb_fit(resp="mpg", expl=c("cyl", "hp", "qsec"),
          max_depth=2, nrounds=30) \%>\%
  xgb_predict(ntrees=20, fns=list(mean=mean))
res \%>\% summarise(pred_metrics(pred_mean, mpg))
}
